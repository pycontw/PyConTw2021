window.__NUXT__=(function(a,b,c,d){return {staticAssetsBase:"\u002Fpycontw-frontend\u002F_nuxt\u002Fstatic\u002F1723713504",layout:"default",error:d,state:{sponsorsData:[],jobsData:[],schedulesData:[],keynotesData:[],youtubeInfo:[],speechesData:[],speechData:{id:326,begin_time:"2024-09-22T05:45:00Z",end_time:"2024-09-22T06:30:00Z",is_remote:c,location:"6-r2",youtube_id:b,title:"Effectively memory profiling distributed PySpark code",category:"DATA",language:"ENEN",python_level:"EXPERIENCED",recording_policy:a,abstract:"When writing code using PySpark to run distributed computations, it can be\r\ndifficult to understand and profile your code since PySpark code executes both\r\nPython and JVM processes, possibly also running native code. This model is very\r\ndifferent to non-distributed code using something like pandas, which runs in\r\nthe same process. This talk will arm you with the knowledge needed to\r\nunderstand the PySpark driver\u002Fworker model, demonstrate how the open source\r\nMemray memory profiler can be used to profile Python and native (C\u002FC++\u002FRust)\r\ncode across drivers and workers, and take a deep dive into some challenging\r\ndata processing scenarios where memory usage comes from unexpected places.",detailed_description:"* [PySpark](https:\u002F\u002Fspark.apache.org\u002Fdocs\u002Flatest\u002Fapi\u002Fpython\u002Findex.html)\r\n  + This talk uses PySpark, which is well known and needs only a brief introduction.\r\n  + As of PySpark 3.4, PySpark includes a memory profiler which allows profiling Python code running on executors.\r\n  + We will compare and contrast this built in memory profiler with Memray.\r\n\r\n* [Memray](https:\u002F\u002Fgithub.com\u002Fbloomberg\u002Fmemray)\r\n  + The focus of this talk is using Memray to profile memory usage in challenging distributed situations.\r\n  + Memray is a relatively new (open sourced in 2022) and not yet widely adopted Python memory profiling tool.\r\n  + One of the key, most innovative features of Memray is that it can seamlessly show memory allocations inside native extensions and can integrate profiles from C, C++, and Rust libraries. This is critical for us to understand memory usage from C extensions, which are common in our high performance data intensive use cases.\r\n\r\n* [pandas](https:\u002F\u002Fpandas.pydata.org\u002F)",slide_link:b,slido_embed_link:b,hackmd_embed_link:b,speakers:[{thumbnail_url:"https:\u002F\u002Ftw.pycon.org\u002Fprs\u002Fmedia\u002Fcache\u002Fba\u002F47\u002Fba474edb56f4ca99bbf0ee08ee0eb6e6.jpg",name:"Kaashif Hymabaccus",github_profile_url:"https:\u002F\u002Fgithub.com\u002Fkaashif",twitter_profile_url:b,facebook_profile_url:b,bio:"Kaashif Hymabaccus is a senior software engineer at Bloomberg. His team builds distributed systems to compute and store portfolio analytics, and he and his teammates are heavy users of Python, pandas, and PySpark."}],event_type:"talk"},relatedData:[],configs:{conferenceName:"PyCon TW",conferenceYear:"2024",conferenceDate:"2024-09-21",showAboutStaffPage:c,showConferencePage:a,showEventOverviewPage:a,showEventsPage:a,showIndexSecondaryBtn:a,showIndexSponsorSection:a,showProposalSystemPage:a,showRegistrationPage:a,showSchedulePage:c,showSpeakingPage:c,showSponsorPage:a,showVenuePage:a,aboutHideItems:["apacCommunity"],conferenceHideItems:["panelDiscussion"],eventsHideItems:["sprints"],registrationHideItems:[],venueHideItems:[]},i18n:{routeParams:{}}},serverRendered:a,routePath:"\u002Fen-us\u002Fconference\u002Ftalk\u002F326",config:{gtm:{id:"GTM-TNZ39PD"},_app:{basePath:"\u002Fpycontw-frontend\u002F",assetsPath:"\u002Fpycontw-frontend\u002F_nuxt\u002F",cdnURL:d}}}}(true,"",false,null));